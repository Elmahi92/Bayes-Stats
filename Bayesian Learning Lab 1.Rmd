---
title: "Bayesian Learning Lab 1"
author: "Mohamed Ali"
date: '2023-05-06'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
library(LaplacesDemon)
```

## Question 1 Daniel Bernoulli 

First we calculate the mean and the standar deviation from the below equation to compare the with sample means and standar deviations of $\theta$ as function of the accumulating number of drawn values.
$$
E(\theta|y) \ =\ a/a+b
$$
$$
E(\theta|y) \ =\ ab/(a+b)^2(a+b+1)
$$
# A
### Figure 1.1


The figure below shows how the values of the Mean and Sd converges to the true values as the number of draws grows large

```{r ber_fun}
ber_fun<-function(n_d,n,s,a,b) {
  #Initial Value of the function givan from the question
  t_n = n
  s   = s
  f   = n-s
  a   = a
  b   = b
  #Beta(alpha+s,beta+s)
  a_new = a+s
  b_new = b+f
  Mean_true= a_new/(a_new+b_new)
  #we take the sqrt to get the Sd insted of the Var
  Sd_true=  sqrt((a_new*b_new)/(((a_new+b_new)^2) * (a_new+b_new+1)))
  # Beta(alpha+s,beta+s)
  mean_theta = c()
  sd_theta = c()
  n_draws = 1:n_d
  #For loop to fill the values of mean and sd based on the draws 
  for (i in 1:n_d){
    mean_theta[i]=mean(rbeta(i,a_new,b_new))
    sd_theta[i]=sd(rbeta(i,a_new,b_new))}
  #Binding everything togther
  df<-cbind.data.frame(n_draws,mean_theta,sd_theta)
  #Plot of the mean 
  mean<-ggplot(df,aes(x=n_draws))+geom_line(aes(y=mean_theta)
                                            , color='#FCA311', size=.8)+
              geom_line(aes(y=Mean_true), color='#14213D',linetype=3)+
              annotate(geom = "text", x = 8, y = Mean_true, 
              label = paste0(format(round(Mean_true, 3), nsmall = 3)))+
              labs(title = 'Sample Means and SD of theta ',
         subtitle = 'As a function of the accumulating number of drawn values',
                          x= ' ', y='Sample Mean')+ theme_classic()
  #Plot of the Sd
  sd<-ggplot(df,aes(x=n_draws))+geom_line(aes(y=sd_theta),
                                          color='#FCA311', size=.8)+
              geom_line(aes(y=Sd_true), color='#14213D',linetype=3)+
              annotate(geom = "text", x = 8, y = Sd_true,
              label = paste0(format(round(Sd_true, 3), nsmall = 3)))+
              labs(x= 'Number of draws', y='Standard Deviation')+
              theme_classic()
  #grid.arrange(mean,sd)
  return(grid.arrange(mean,sd))
}


ber_fun(100,70,22,8,8)
```

# B

First we find the value from the beta posterior using the function $pbeta$ we get the value of theta which can be used to  compute the probability that a random variable from a beta distribution is less than or equal to a given value, or greater than a given value, depending on the value of the lower.tail argument. in our case we use lower.tail as False because we need the values grater than.

```{r prob, echo=FALSE}
prob<-function(n_d,n,s,a,b,prob){
        t_n = n
        s   = s
        f   = n-s
        a   = a
        b   = b
        #Beta(alpha+s,beta+s)
        a_new = a+s
        b_new = b+f
        p<-pbeta(prob,a_new,b_new,lower.tail = F)
        post<-mean(rbeta(n_d,a_new,b_new)>prob)
        return(list(paste('random values from the posterior with the given condition',post)
        ,paste('The exact value from the Beat posterior', p)))}
prob(100,70,22,8,8,.3)
```

# C
Now we want to draw 10000 random values from the posterior of the odds

```{r grp, echo=FALSE}
grp<-function(n_d,n,s,a,b){
        t_n = n
        s   = s
        f   = n-s
        a   = a
        b   = b
        #Beta(alpha+s,beta+s)
        a_new = a+s
        b_new = b+f
        res<-rbeta(n_d,a_new,b_new)
        res2<-data.frame(x=log(res/(1-res)))
        plt<-ggplot(res2,aes(x=x))+geom_histogram(aes(y=..density..),
                                      color="white",linetype=6,
                                      fill='#14213D',binwidth = 0.05)+
                                      labs(x='Odds values',y=' ',
                                      title ='Posterior of the odds values')+
                                      stat_function(fun = dnorm, 
                                          args = list(mean = mean(res2$x),
                                          sd = sd(res2$x)),
                                          color = "#FCA311", size = 1)
        return(plt)}

grp(10000,70,22,8,8)
```


## Question 2 Log-Normal distribution and Gini Coefficient

# A

First we find the value of tau, the question assume that we have 8 randomly selected persons thus we have n = 8 and we have the value of mu = 3.6 Given, to find the value of tau we use this equation: $$\tau^2\ =\ \sum_1^n(log\ y_i \ - \mu)^2/n$$'.
*Note*
A non-informative prior is a prior distribution that is chosen to express little to no prior information about the parameters.
Non-informative priors are often chosen to avoid introducing bias or strong assumptions into the model, and to allow the data to have a greater influence on the posterior distribution. Examples of non-informative priors include the uniform distribution, the Jeffreys prior, and the reference prior.
It's important to note that a non-informative prior is not necessarily a prior with no information at all, but rather one that expresses a minimal amount of information that is consistent with our knowledge and beliefs before observing the data. In practice, the choice of prior distribution often depends on the specific problem and the available prior knowledge.

in the question we have inverse chi distribution is our posterior,  the inverse-chi-squared distribution (or inverted-chi-square distribution[1]) is a continuous probability distribution of a positive-valued random variable. It is closely related to the chi-squared distribution. It arises in Bayesian inference, where it can be used as the prior and posterior distribution for an unknown variance of the normal distribution.

the inverse chi distribution is The inverse chi-square distribution with degrees of freedom n and scale parameter $s^2$ is closely related to the inverse gamma distribution with shape parameter $\alpha$ = n/2 and scale parameter $\beta$ = 1/(2$s^2$). In fact, if X is Inv-$\chi^2$(n,$s^2$) distributed, then Y = (n$s^2$)/X is Inv-$\gamma$($\alpha$,$\beta$) distributed, and vice versa.
then we use the function rinvgamma from r and we change on the pramters shape and scale.

```{r lognormal, echo=FALSE}
y<-c(33,24,48,32,55,74,23,17)
#n is the sample size
n=8
#the number of draws wanted
nDraws=10000
mu= 3.6
logy= (log(y)-mu)^2
tau2=sum(logy)/n
# then we use the function rinvgamma from r and we change on the pramters shape and scale.
df<-data.frame(x=rinvgamma(nDraws, shape=n/2, scale=n*tau2/2))
plt<-ggplot(df,aes(x=x))+geom_histogram(aes(y=..density..),color="white",
                                            linetype=6,
                                      fill='#14213D',binwidth = 0.05)+
                                            labs(x='Estimate Values',y=' ',
                                            title ='Posterior distribution')+
                                            xlim(0,1)+ 
  geom_density(color = "#FCA311", size = 1)
plt
```

# B

We define  The Gini coefficient as a measure of inequality in a distribution, typically used to measure income inequality. It ranges from 0 (perfect equality, where everyone has the same income) to 1 (perfect inequality, where one person has all the income). A Gini coefficient of 0.5, for example, indicates that 50% of the population has 50% of the total income, while the other 50% of the population has the remaining 50% of the income.

*Steps*:

1- We find the value of $\phi(\sigma/\sqrt2)$ by using the values of $\sigma^2$ from the estimated values in A using the formula $\sigma/\sqrt2$.
2- We use the function $pnorm$ to find the values of $\phi$ where:
  - q: the quantile(s) at which to evaluate the CDF.
mean: the mean of the normal distribution (default value is 0).
  - sd: the standard deviation of the normal distribution (default value is 1).
  - lower.tail: a logical value indicating whether to compute the lower tail probability (TRUE, default) or the upper tail probability (FALSE).
  - log.p: a logical value indicating whether to return the natural logarithm of the probability density (TRUE) or the probability density (FALSE, default).
*Note that*:
Quantiles are points in a probability distribution that divide the distribution into intervals of equal probability.In our case we use the probabilities from the function $x=rinvgamma(nDraws, shape=n/2, scale=n*tau2/2)$

3- We calculate the value of G by pluggin 1 and 2

```{r Gini, echo=FALSE}
y<-c(33,24,48,32,55,74,23,17)
#n is the sample size
n=length(y)
#the number of draws wanted
nDraws=10000
mu= 3.6
logy= (log(y)-mu)^2
tau2=sum(logy)/n
# then we use the function rinvgamma from r and we change on the pramters shape and scale.
df<-data.frame(x=rinvgamma(nDraws, shape=n/2, scale=n*tau2/2))
# Calculating the value of Phi_arg which is simply the squar root of the estimated sigma
df$phi_arg<-df$x/sqrt(2)
# Calculating the value of Gini index by appling the formula G=2*phi(sigma/sqrt(2))-1
df$G<-2*pnorm(sqrt(df$phi_arg),0,1)-1
#Ploting the Gini distribution
ggplot(df,aes(x=G))+geom_histogram(aes(y=..density..),color="white",
                                   linetype=6,
                                   fill='#14213D',binwidth = 0.05)+
  labs(x='Estimate Values',y=' ',
       title ='Distribution of Gini Coefficient')+
  geom_density(color = "#FCA311", size = 1)
```
# C
To find the 95% CI we use the function $qnorm()$in R with mean and Sd drived from the Gini distribution we found in the previous data.
here we indicates that 0,025 is the 2,5% cut off point for both uppbe and lower limits.

```{r CI, echo=FALSE}
sample_mean= mean(df$G)
sample_sd= sd(df$G)
# Compute the 95% confidence interval
lower_ci <- qnorm(0.025, mean = sample_mean, sd = sample_sd)
upper_ci <- qnorm(1-0.025, mean = sample_mean, sd = sample_sd)

# Print the confidence interval
cat("95% confidence interval: [", round(lower_ci, 2), ",", round(upper_ci, 2), "]", "\n")

```

# D
We define the Highest Posterior Density (HPD) interval is a type of confidence interval in Bayesian inference that contains the most credible values for a parameter based on the observed data. Specifically, it is the narrowest interval that contains a specified proportion (usually 95% or 99%) of the posterior distribution of the parameter.
To do so First we generate a sample using the posterior mean and Sd then We use the function HDI to find the 95% CI from library(HDInterval)

```{r HDPI, echo=FALSE}
library(HDInterval)

#First we generate a sample using the posterior mean and Sd
post_G <- rnorm(n = 10000, mean = mean(df$G), sd = sd(df$G))
post_G_density <- density(post_G)

#We use the function HDI to find the 95% CI 
hdpi=hdi(post_G, conf = 0.95)

cat("95% equal tail interavl for G : [", round(lower_ci, 2), ",", round(upper_ci, 2), "]", "\n")

cat("95% Highest Posterioe Density Interval for G: [", round(hdpi[1], 2), ",", round(hdpi[2], 2), "]", "\n")

```

## Question 3 Bayesian inference for the concentration parameter in the von Mises distribution

# A
First to we drive our joint distribution to find the liklehood function from the model distribution.
We have our model distribution $p(y|\mu,k)$ defined as:
$$
P(y|\mu,k)=\prod_{i=1}^n\ \frac{1}{2\pi I_0(k)}*exp(k*Cos(y-\mu))\\= \frac{1}{(2\pi)^n I_0(k)^n}* exp(k*Cos(\sum_{i=1}^n y_i-\mu))
$$
We have the prior k follows the $exp(\lambda=0.5)$
$$
p(k)=0.5*exp(-\lambda*k)
$$
now we drive the postrior distribution using bayse formula
$$
p(\theta|x)\propto p(x|\theta)\ p(\theta)
$$
We plug the liklehood function drived above with the prior we get:
$$
p(k|\mu,y)\propto\frac{1}{(2\pi)^n I_0(k)^n}* exp(k*Cos(\sum_{i=1}^n y_i-\mu))\ .\ 0.5*exp(-\lambda*k) \\
\propto \frac{1}{I_0(k)^n}* exp(k*Cos(\sum_{i=1}^n y_i-\mu)- \frac{k}{2})
$$
Now we implement the code in R:

```{r k, echo=FALSE}
# The y and mu values given from the question 
y<-c(-2.79,2.33,1.83,-2.44,2.23,2.33,2.07,2.02,2.14,2.54)
mu<- 2.51
n=length(y)

#We generate a sequeance of k values by 0.1
k<-seq(0.01,10,by=0.01)

#The drived posterior function can be expressed by 
#(0.5/(besselI(k,0))^n)*exp(k*sum(cos(y-mu))-(k*0.5)
df<- data.frame(x=k,y=(0.5/(besselI(k,0))^n)*exp(k*sum(cos(y-mu))-(k*0.5)))

#Plotting function:
plt<-ggplot(df,aes(x=x,y=y))+geom_line(aes(),
                                          color="#FCA311",linetype=1,size=1)+
  labs(x='k values',y=' ',
       title ='The posterior distribution of k for the wind direction data')+
         geom_vline(xintercept =df$x[which.max(df$y)],
                    # we use df$x[which.max(df$y)] to find the mode
                    linetype = "dashed", color = "#14213D",size=.8)
plt



```

