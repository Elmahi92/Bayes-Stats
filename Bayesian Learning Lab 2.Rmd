---
title: "Bayesian learning Lab 2"
author: "Mohamed Ali"
date: '2023-05-10'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(mvtnorm)
library(readxl)
library(LaplacesDemon)
```

## Linear and polynomial regression

To answer this question a conjugate prior for the linear regression model will be used in which: 
The joint prior for $\beta \ and\  \sigma^2$:
$$
\begin{aligned}
\beta|\sigma^2 \sim \ \mathcal{N}(\mu_0,\sigma^2\Omega{_0}^{-1})\\ \sigma^2 \sim Inv-\chi^2(v_0,\sigma^2)
\end{aligned}
$$
While the posterior:
$$
\begin{aligned}
\beta|\sigma^2,y \sim \ \mathcal{N}(\mu_n,\sigma^2\Omega{_n}^{-1})\\ 
\sigma^2 \sim Inv-\chi^2(v_n,\sigma{^2}{_n})\\
where:\\
\mu_n= (X'X\Omega_0)^{-1}\ (X'X\hat\beta+\Omega_0\mu_0)\\
\Omega_n=X'X+\Omega_0\\
v_n= v_0+n\\
v_n\sigma^2=v_n\sigma^2+(y'y+\mu_0'\Omega_0\mu_0-\mu_n'\Omega_n\mu_n)
\end{aligned}
$$


First we start by reading the files and then we Create the covariate_time vaiable as (the number of days sinvce the beginning of the year / 365)
```{r data}
#reading the files 
df<-read_xlsx("Linkoping2022.xlsx")
#Creating the covariate_time vaiable as 
#(the number of days sinvce the beginning of the year / 365)  
a<- df$datetime
#begunning of the year
b<- '2022-01-01'
a<-format.POSIXlt(strptime(a,'%Y-%m-%d'))
b<-format.POSIXlt(strptime(b,'%Y-%m-%d'))
#time diff from the 
x<-as.vector(difftime(a,b,units='days'))
df$cov_tm<-x/365
```


## A

We assume to use a conjugate prior from the linear regression in Lec 5 , we have been given the prior hyperparamteres as follow:

```{r par}
#We assume to use a conjugate prior from 
#the linear regression in Lec 5 , 
#we have been given the prior hyperparamteres as follow:
mu_0= as.matrix(c(0,100,-100),ncol=3)
omega_0=0.01*diag(3)
v_0=1
segma2_0=1
n= length(df$temp)
ndraws=10

```

We have the joint prior for beta and segma2 defined as $\beta|\sigma^2\sim\mathcal{N}(\mu_0,\sigma^2\Omega_{_0}^{-1})$  and $\sigma^2 \sim Inv-\chi^2(v_o,\sigma^2_0)$ follows Inv-Chi(v0,sigma2_0)

First we draw our random samaple from inv-chi2 using the below defined function from Lec 3 slide 5

```{r draws, echo=FALSE}
# Step 1: Draw X ~ χ²(n - 1)
draw_chi_sq <- function(n) {
  return(rchisq(1, df = n - 1))
}
# Step 2: Compute σ² = (n - 1) * s² / X
compute_sigma_sq <- function(n, segma2_0, X) {
  return((n - 1) * segma2_0 / X)
}

# simulation
segma_estimation <- function(n, mu_0, segma2_0, ndraws) {
  results <- c()
  
  for (i in 1:ndraws) {
    X <- draw_chi_sq(n)
    sigma_sq <- compute_sigma_sq(n, segma2_0, X)
    results[i] <- sigma_sq
  }
  return(results)
}

sigma2<-segma_estimation(n, mu_0, segma2_0, ndraws)
```

Now we estimate the betas values using the formula $\beta|\sigma^2\sim\mathcal{N}(\mu_0,\sigma^2\Omega_{_0}^{-1})$ and we fit the regression based ontemp = beta0 + beta1 time + beta 2 time^2 + erorr

*Note  we have our error follows the normal distrbution by 0 and $\sigma^2$
```{r sigma estimates ,echo=FALSE}
for (i in 1:length(sigma2)) {
  e<- rnorm(1,0,sigma2[i])
  res<-rmvnorm(1,mu_0,sigma2[i]*omega_0)
  temp= x=res[1,1]+res[1,2]*df$cov_tm+res[1,3]*df$cov_tm^2+e
  df[[paste0("temp_fit_",sigma2[i])]]<-temp 
}
```

After we done with the draws now for every draw compute the regression curve
```{r plot,echo=FALSE,fig.align='center'}
# Define a vector of colors
colors <-palette('Tableau')
  
  #c("#FCA311", "#00FF00", "#0000FF", "#FFFF00", "#00FFFF",
   #         "#FF00FF", "#800000", "#008000", "#000080", "#808000",
    #        "#800080", "#008080", "#808080", "#FFC0CB", "#FFA500",
     #       "#FFD700", "#A52A2A", "#7FFF00", "#FF1493", "#00BFFF")

# Plot with different colored lines
plt <- ggplot(df, aes(x = cov_tm, y = temp)) +
  geom_point(aes(color = factor('temp')), size = 1)

for (i in names(df)[-c(1:4, ncol(df))]) {
  plt <- plt + geom_line(aes_string(y = i, color = factor(i)), linetype = 1)
}

# Map colors to the lines
plt <- plt +
  scale_color_manual(values = colors) +
  labs(x = 'time', y = 'temp',color='Predictions with different Segma values') +
  theme_classic()
plt
```

Comparing the above chart with our  prior beliefs which we can see by running the polynomial model using the *lm* function in R with df = 2 we can see the fitted regression line with to some extend follow our regression line when $\sigma^2$ is equal to *1.037*.

```{r ploy,echo=FALSE,fig.align='center'}
degree <- 2  # Set the degree of the polynomial
x= df$cov_tm
y= df$temp
model <- lm(y ~ poly(x, degree, raw = TRUE))
df_plt<- data.frame(x=x,y=y)
z=predict(model)
# Print the model summary


# Plot the data and regression line
plt <- ggplot(df_plt, aes(x = x, y = y)) +
  geom_point(color = "#4E79A7", size = 1)+
  geom_line(aes(y = z), color = "#FCA311", linetype = 1)+
  labs(x = 'time', y = 'temp'
       ,title ='Polynomial Regression with Dgree 2') +
  theme_classic()
plt
summary(model)

```


## B

As we want to estimate the uncertainty in the model parameters. Simulating from the joint posterior allows us to obtain a set of plausible values for all the parameters in the model, taking into account the observed data and prior information.
to do so:
we have our non-informative prior:
$$
\begin{aligned}
p(\beta,\sigma^2) \propto \sigma^-1
\end{aligned}
$$
Our joint posterior of $\beta$ and $\sigma^2$:
$$
\begin{aligned}
\beta|\sigma^2,y \sim \mathcal{N}(\hat\beta,\sigma^2(X´X)^{-1})\\
\sigma^2|y \sim Inv-\mathcal{\chi^2}(n-k,s^2)\\
where:\\
k=number\ of \ \beta s\ in \ our \ case \ 2\\
\hat\beta= (X´X)^{-1} X´y\\
s^2= \frac{1}{n-k}(y-X\hat\beta)´(y-X\hat\beta)
\end{aligned}
$$
Thus to simulate from the joint posterior we need to simulate from:
1- $p(\sigma|y$
2- $p(\beta|\sigma^2,y)$

And then we find the marginal posterior of $\beta$:
$$\beta|y\sim t_n-k(\hat\beta,s^2(X´X)^-1)$$
There for to draw a sample from the joint posterior distribution of $\beta_0,\beta_2,\beta_3\ and \ \sigma^2$: 
we need first to calculate the value of $\hat\beta= (X´X)^-1X´y$
```{r beta_hat,echo=FALSE}
y<-as.matrix(df$temp)
x<-as.matrix(df$cov_tm)
n<- length(y)
k=2
beta_ht<-(solve(t(x)%*%x))%*%(t(x)%*%y)
```

Then we calculate the value of $s^2= \frac{1}{n-k}(y-X\hat\beta)´(y-X\hat\beta)$

```{r s2,echo=FALSE}
s2<-(1/n-k) %*% t((y-(x%*%beta_ht)))%*%(y-(x%*%beta_ht))
```

Now we generate  $\sigma^2$ from $\sigma^2|y \sim Inv-\mathcal{\chi^2}(n-k,s^2)$, we have that $Inv-\mathcal{\chi^2}$ in R is *rinvchisq*.

```{r sigma2,echo=FALSE}
df_<-(n-k)
lmbda_<-s2
sigma2<-rinvchisq(1,df_,lmbda_)
```




